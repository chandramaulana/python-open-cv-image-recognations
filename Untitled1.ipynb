{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2a344d7-9bfc-44e6-a16c-dc05a86441ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\python\\Face_Images\n",
      "C:\\Users\\hp\\python\\Face_Images\\chandra\\1.jpeg chandra\n",
      "0 ()\n",
      "C:\\Users\\hp\\python\\Face_Images\\chandra\\2.jpeg chandra\n",
      "0 ()\n",
      "C:\\Users\\hp\\python\\Face_Images\\chandra\\3.jpeg chandra\n",
      "0 ()\n",
      "C:\\Users\\hp\\python\\Face_Images\\chandra\\4.jpeg chandra\n",
      "0 [[170 154 182 182]]\n",
      "C:\\Users\\hp\\python\\Face_Images\\chandra\\5.jpeg chandra\n",
      "0 ()\n",
      "C:\\Users\\hp\\python\\Face_Images\\dwi_pras\\1.jpeg dwi_pras\n",
      "1 [[156 150 273 273]]\n",
      "C:\\Users\\hp\\python\\Face_Images\\dwi_pras\\2.jpeg dwi_pras\n",
      "1 ()\n",
      "C:\\Users\\hp\\python\\Face_Images\\dwi_pras\\3.jpeg dwi_pras\n",
      "1 [[140 158 273 273]]\n",
      "C:\\Users\\hp\\python\\Face_Images\\dwi_pras\\4.jpeg dwi_pras\n",
      "1 ()\n",
      "C:\\Users\\hp\\python\\Face_Images\\dwi_pras\\5.jpeg dwi_pras\n",
      "1 [[127 161 273 273]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import cv2 #For Image processing \n",
    "import numpy as np #For converting Images to Numerical array \n",
    "import os #To handle directories \n",
    "from PIL import Image #Pillow lib for handling images \n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "\n",
    "Face_ID = -1 \n",
    "pev_person_name = \"\"\n",
    "y_ID = []\n",
    "x_train = []\n",
    "\n",
    "Face_Images = os.path.join(os.getcwd(), \"Face_Images\") #Tell the program where we have saved the face images \n",
    "print (Face_Images)\n",
    "\n",
    "for root, dirs, files in os.walk(Face_Images): #go to the face image directory \n",
    "\tfor file in files: #check every directory in it \n",
    "\t\tif file.endswith(\"jpeg\") or file.endswith(\"jpg\") or file.endswith(\"png\"): #for image files ending with jpeg,jpg or png \n",
    "\t\t\tpath = os.path.join(root, file)\n",
    "\t\t\tperson_name = os.path.basename(root)\n",
    "\t\t\tprint(path, person_name)\n",
    "\n",
    "\t\t\t\n",
    "\t\t\tif pev_person_name!=person_name: #Check if the name of person has changed \n",
    "\t\t\t\tFace_ID=Face_ID+1 #If yes increment the ID count \n",
    "\t\t\t\tpev_person_name = person_name\n",
    "\n",
    "\t\t\t\n",
    "\t\t\tGery_Image = Image.open(path).convert(\"L\") # convert the image to greysclae using Pillow\n",
    "\t\t\tCrop_Image = Gery_Image.resize( (550,550) , Image.ANTIALIAS) #Crop the Grey Image to 550*550 (Make sure your face is in the center in all image)\n",
    "\t\t\tFinal_Image = np.array(Crop_Image, \"uint8\")\n",
    "\t\t\t#print(Numpy_Image)\n",
    "\t\t\tfaces = face_cascade.detectMultiScale(Final_Image, scaleFactor=1.5, minNeighbors=5) #Detect The face in all sample image \n",
    "\t\t\tprint (Face_ID,faces)\n",
    "\n",
    "\t\t\tfor (x,y,w,h) in faces:\n",
    "\t\t\t\troi = Final_Image[y:y+h, x:x+w] #crop the Region of Interest (ROI)\n",
    "\t\t\t\tx_train.append(roi)\n",
    "\t\t\t\ty_ID.append(Face_ID)\n",
    "\n",
    "recognizer.train(x_train, np.array(y_ID)) #Create a Matrix of Training data \n",
    "recognizer.save(\"face-trainner.yml\") #Save the matrix as YML file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7febb258-f13f-41b7-a3e0-3e5877dac17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Program to Detect the Face and Recognise the Person based on the data from face-trainner.yml\n",
    "\n",
    "import cv2 #For Image processing \n",
    "import numpy as np #For converting Images to Numerical array \n",
    "import os #To handle directories \n",
    "from PIL import Image #Pillow lib for handling images \n",
    "\n",
    "labels = [\"Chandra Maulana\", \"Dwi Pras\"] \n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "recognizer.read(\"face-trainner.yml\")\n",
    "\n",
    "cap = cv2.VideoCapture(0) #Get vidoe feed from the Camera\n",
    "\n",
    "while(True):\n",
    "\n",
    "    ret, img = cap.read() # Break video into frames \n",
    "    gray  = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) #convert Video frame to Greyscale\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.5, minNeighbors=5) #Recog. faces\n",
    "    for (x, y, w, h) in faces:\n",
    "    \troi_gray = gray[y:y+h, x:x+w] #Convert Face to greyscale \n",
    "\n",
    "    \tid_, conf = recognizer.predict(roi_gray) #recognize the Face\n",
    "    \n",
    "    \tif conf>=80:\n",
    "    \t\tfont = cv2.FONT_HERSHEY_SIMPLEX #Font style for the name \n",
    "    \t\tname = labels[id_] #Get the name from the List using ID number \n",
    "    \t\tcv2.putText(img, name, (x,y), font, 1, (0,0,255), 2)\n",
    "    \t\n",
    "    \tcv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "\n",
    "    cv2.imshow('Preview',img) #Display the Video\n",
    "    if cv2.waitKey(20) & 0xFF == ord('q'):\n",
    "    \tbreak\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566e6880-de47-4419-a1b7-a682cde29e43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
